{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training with SimpleDBpediaQA\n",
    "\n",
    "- Dataset repository: [castorini/SimpleDBpediaQA](https://github.com/castorini/SimpleDBpediaQA)\n",
    "- Stats (forward only):\n",
    "  - 3,777 training questions\n",
    "  - 500 validation questions\n",
    "  - 1,000 test questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Prerequisite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import dependencies:\n",
    "\n",
    "```bash\n",
    "pip install srsly srtk pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import srsly\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from GitHub to `data/dbpedia-qa/raw` with the following commands:\n",
    "\n",
    "```bash\n",
    "mkdir -p data/dbpedia-qa/raw\n",
    "wget https://raw.githubusercontent.com/castorini/SimpleDBpediaQA/master/V1/train.json -P data/dbpedia-qa/raw\n",
    "wget https://raw.githubusercontent.com/castorini/SimpleDBpediaQA/master/V1/valid.json -P data/dbpedia-qa/raw\n",
    "wget https://raw.githubusercontent.com/castorini/SimpleDBpediaQA/master/V1/test.json -P data/dbpedia-qa/raw\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Format the Raw Data for Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Inspect raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_root = 'data/dbpedia-qa/raw'\n",
    "splits = ['train', 'valid', 'test']\n",
    "raw_paths = {split: os.path.join(raw_root, f'{split}.json') for split in splits}\n",
    "\n",
    "intermediate_dir = 'data/dbpedia-qa/intermediate' # intermediate data, here it's the scored paths\n",
    "dataset_dir = 'data/dbpedia-qa/dataset'  # preprocessed data\n",
    "output_model_dir = 'artifacts/models/dbpedia_qa'\n",
    "retrieved_subgraph_path = 'artifacts/subgraphs/dbpedia_qa.jsonl'\n",
    "\n",
    "# Create directories\n",
    "Path(intermediate_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(dataset_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(output_model_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"DatasetName\": \"SimpleDBpediaQA-TRAIN\",\n",
      "  \"Questions\": [\n",
      "    {\n",
      "      \"ID\": \"00007\",\n",
      "      \"Query\": \"what movie is produced by warner bros.\",\n",
      "      \"Subject\": \"http://dbpedia.org/resource/Warner_Bros.\",\n",
      "      \"FreebasePredicate\": \"www.freebase.com/film/production_company/films\",\n",
      "      \"PredicateList\": [\n",
      "        {\n",
      "          \"Predicate\": \"http://dbpedia.org/ontology/distributor\",\n",
      "          \"Direction\": \"backward\",\n",
      "          \"Constraint\": \"http://dbpedia.org/ontology/Film\"\n",
      "        }\n",
      "      ]\n",
      "    },\n"
     ]
    }
   ],
   "source": [
    "raw_train_path = raw_paths['train']\n",
    "! head -n 16 $raw_train_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Remove backward relations\n",
    "\n",
    "Currently, our retrieval model can only handle forward relations. So we need to remove backward relations from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train size: 30186\n",
      "train size after removing reverse relation: 16000\n",
      "53.00% percentage train data are kept\n",
      "Full train size: 4305\n",
      "valid size after removing reverse relation: 2310\n",
      "53.66% percentage valid data are kept\n",
      "Full train size: 8595\n",
      "test size after removing reverse relation: 4608\n",
      "53.61% percentage test data are kept\n"
     ]
    }
   ],
   "source": [
    "preserved_data = {}\n",
    "for split in splits:\n",
    "    raw_path = raw_paths[split]\n",
    "    data = srsly.read_json(raw_path)['Questions']\n",
    "    print('Full train size:', len(data))\n",
    "    before_len = len(data)\n",
    "    # Remove samples where the Direction are all 'reverse'\n",
    "    data = [sample for sample in data if any(p['Direction'] == 'forward' \n",
    "                                             and p['Predicate'].startswith('http://dbpedia.org/ontology/')\n",
    "                                             for p in sample['PredicateList'])]\n",
    "    after_len = len(data)\n",
    "    print(f'{split} size after removing reverse relation: {len(data)}')\n",
    "    print(f'{after_len/before_len*100:.2f}% percentage {split} data are kept')\n",
    "    preserved_data[split] = data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Convert to scored path format\n",
    "\n",
    "The paths format is a JSONL file, where each line is a dictionary as:\n",
    "```json\n",
    "{\n",
    "    \"id\": \"train-100\",\n",
    "    \"question\": \"What is the birth place of Barack Obama?\",\n",
    "    \"question_entities\": [\"Q76\"],\n",
    "    \"paths\": [[\"P19\"]]  # there may be multiple paths, and each path may have variable lengths\n",
    "    \"scores\": [1.0]     # the score of each path. for ground truth paths, we assign max score 1.0 to each path.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 16000/16000 [00:00<00:00, 69926.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train scored paths file to data/dbpedia-qa/intermediate/scores_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing valid: 100%|██████████| 2310/2310 [00:00<00:00, 125726.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved valid scored paths file to data/dbpedia-qa/intermediate/scores_valid.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 4608/4608 [00:00<00:00, 120893.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test scored paths file to data/dbpedia-qa/intermediate/scores_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    data = preserved_data[split]\n",
    "    samples = []\n",
    "    for sample in tqdm(data, desc=f'Processing {split}'):\n",
    "        question_entity = sample['Subject'].split('/')[-1]\n",
    "        relations = [[p['Predicate'].split('/')[-1]] for p in sample['PredicateList']\n",
    "                     if p['Direction'] == 'forward' and p['Predicate'].startswith('http://dbpedia.org/ontology/')]   \n",
    "        question = sample['Query']\n",
    "        idx = sample['ID']\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"question\": question,\n",
    "            \"question_entities\": [question_entity],\n",
    "            \"paths\": relations,\n",
    "            \"path_scores\": [1.0]\n",
    "        }\n",
    "        samples.append(sample)\n",
    "    save_path = os.path.join(intermediate_dir, f'scores_{split}.jsonl')\n",
    "    srsly.write_jsonl(save_path, samples)\n",
    "    print(f'Saved {split} scored paths file to {save_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect a train sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"00012\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"question\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Which city did the artist ryna originate in\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"question_entities\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "    \u001b[0;32m\"RYNA\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"paths\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "    \u001b[1;39m[\n",
      "      \u001b[0;32m\"hometown\"\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"path_scores\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "    \u001b[0;39m1\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 data/dbpedia-qa/intermediate/scores_train.jsonl | jq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preprocessing\n",
    "\n",
    "Use the `srtk preprocess` command to creating training samples. Do not pass `--search-path` beacuse the paths are already provided in the dataset. This step mainly involves negative sampling and dataset generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train data...\n",
      "Negative sampling: 100%|█████████████████| 16000/16000 [01:12<00:00, 219.56it/s]\n",
      "Number of training records: 0\n",
      "Converting relation ids to labels: 0it [00:00, ?it/s]\n",
      "Training samples are saved to data/dbpedia-qa/dataset/train.jsonl\n",
      "Processing valid data...\n",
      "Negative sampling: 100%|███████████████████| 2310/2310 [00:10<00:00, 224.15it/s]\n",
      "Number of training records: 0\n",
      "Converting relation ids to labels: 0it [00:00, ?it/s]\n",
      "Training samples are saved to data/dbpedia-qa/dataset/valid.jsonl\n",
      "Processing test data...\n",
      "Negative sampling: 100%|███████████████████| 4608/4608 [00:18<00:00, 254.34it/s]\n",
      "Number of training records: 0\n",
      "Converting relation ids to labels: 0it [00:00, ?it/s]\n",
      "Training samples are saved to data/dbpedia-qa/dataset/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    print(f'Processing {split} data...')\n",
    "    scored_path = os.path.join(intermediate_dir, f'scores_{split}.jsonl')\n",
    "    dataset_path = os.path.join(dataset_dir, f'{split}.jsonl')\n",
    "    !srtk preprocess -i $scored_path \\\n",
    "        -o $dataset_path \\\n",
    "        -e http://localhost:8890/sparql \\\n",
    "        -kg dbpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 data/dbpedia-qa/dataset/train.jsonl | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dragon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
