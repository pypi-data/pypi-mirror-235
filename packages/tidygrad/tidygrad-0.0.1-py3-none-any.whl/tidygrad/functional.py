# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_funcitonal.ipynb.

# %% auto 0
__all__ = ['sigmoid', 'relu']

# %% ../nbs/02_funcitonal.ipynb 3
from .tensor import Tensor, BaseOp, UnaryElementwiseOp, BinaryElementwiseOp
import numpy as np

# %% ../nbs/02_funcitonal.ipynb 4
class Sigmoid(UnaryElementwiseOp):
    """Take the sigmoid of a tensor"""

    name_template = "sigmoid({})"

    def __init__(self, a, name=None):
        super().__init__(a, name=name)
        self.out = Tensor(1 / (1 + np.exp(-self.args[0].data)), name=self.name, op=self)

    def backward(self):
        self.parents[0].grad += self.out.grad * self.out.data * (1 - self.out.data)

# %% ../nbs/02_funcitonal.ipynb 5
def sigmoid(input, name=None):
    return Sigmoid(input, name=name).out

# %% ../nbs/02_funcitonal.ipynb 6
class Relu(UnaryElementwiseOp):
    """Take the sigmoid of a tensor"""

    name_template = "relu({})"

    def __init__(self, a, name=None):
        super().__init__(a, name=name)
        self.out = Tensor(np.maximum(0, self.args[0].data), name=self.name, op=self)

    def backward(self):
        self.parents[0].grad += self.out.grad * (self.out.data > 0)

# %% ../nbs/02_funcitonal.ipynb 7
def relu(input, name=None):
    return Relu(input, name=name).out
