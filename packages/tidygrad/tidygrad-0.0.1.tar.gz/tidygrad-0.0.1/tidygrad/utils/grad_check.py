# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10_utils.grad_check.ipynb.

# %% auto 0
__all__ = ['grad_check']

# %% ../../nbs/10_utils.grad_check.ipynb 3
from ..tensor import Tensor
from ..functional import sigmoid
import numpy as np

# %% ../../nbs/10_utils.grad_check.ipynb 4
def grad_check(func, inputs, params: tuple = (), eps=1e-5, n=1000):
    for p in reversed(params):
        # Reshape to 1D so it's easier to sample random indices

        data_view = p.data.reshape(-1)  # This does not make a copy
        grad_view = p.grad.reshape(-1)

        slow_grad = np.zeros_like(grad_view)
        slow_grad_view = slow_grad.reshape(-1)

        indices = np.random.choice(
            np.arange(grad_view.size), size=min(n, grad_view.size), replace=False
        )

        indices = list(filter(lambda idx: abs(grad_view[idx]) > 1e-9, indices))  # XXX?
        for idx in indices:
            old_val = data_view[idx]

            loss = func(inputs, params)

            data_view[idx] = old_val + eps
            loss_plus_h = func(inputs, params)

            slow_grad_view[idx] = (loss_plus_h.data - loss.data) / eps
            data_view[idx] = old_val

        max_grad_diff = np.max(
            np.abs(
                (slow_grad_view[indices] - grad_view[indices])
                / np.maximum(slow_grad_view[indices], grad_view[indices])
            )
        )

        print(f"Max gradient difference for {p.name}: {max_grad_diff*100:.4f}%")
        if max_grad_diff > 1e-2:
            raise ValueError(
                f"Gradient check failed for {p.name}: Max error: {max_grad_diff*100:.4f}"
            )
