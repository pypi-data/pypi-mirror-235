[benchsite]
; This section is used to define the website configuration
; The name of the website
name = NameWebsite
; this short description will be used in the meta description tag and the main page
description =  Description of the website
; social media links -> (name of the social media url) and separated by a coma
social_media= discord #,github #,gitlab #
; google analytics key
; google_analyse_key = UA-XXXXXXXXX-X
; the name of the file containing the logo of the website ( inside res )
logo = logo.png
; the name of the file containing the favicon of the website ( inside res )
favicon = favicon.ico
; default_graph_scale and default_post_task_scale (auto, linear, log)
default_task_scale = auto

[benchmark]
; This section is used to define the global configuration of the project
; if nb_runs is not defined in the config file of the task, this value will be used
; nb of tests for each algorithm for each run
default_nb_runs = 1
; if timeout is not defined in the config file of the task, this value will be used
; timeout in seconds ( does not count the time of the before_task )
default_timeout = 120
; if stop_after_x_timeout is not defined in the config file of the task, this value will be used
; stop the task after x timeout
default_stop_after_x_timeout = 3
|[task]
; the title of the task
title = Quick description of the task

; whether the task is active or not 
; (the benchmark will not run it if it is not active but it will be displayed if there are results)
active = True

; nb of tests for each algorithm for each run
nb_runs = 5

; arguments for the task
arguments = 1000,2000,3000,4000,5000

; timeout in seconds ( does not count the time of the before_task )
timeout = 60
stop_after_x_timeout = 3

; the evaluation script name ( inside the task folder ) separeted by a coma
evaluation_scripts = evaluation1.py,evaluation2.py
; the evaluation title separeted by a coma
evaluation_titles = Title1,Title2

; the order of the evaluation for ranking the algorithms ( asc or desc )
evaluation_metric_type = asc,desc; ou un meilleur nom

; the scale of the task graph (auto,linear,log)
perf_scale = auto
; label for x axis
perf_xlabel = label for x axis
; (line,scatter,bar)
perf_display = line

; list of titles of the evalution graphs
evaluation_title = Title1,Title2
; list of scales of the evaluation graphs (auto,linear,log)
evaluation_scale = linear,log
evaluation_xlabel = label for x axis,label for x axis
evaluation_ylabel = label for y axis,label for y axis
; (line,scatter,bar)
evaluation_display = line,bar
|[target]
; the title of the target
title = Name of the target
; the name of the file containing the logo of the target ( inside res )
logo = logo.png
; the command to run the upgrade / install of the target
upgrade = pip install -U <name>
|[theme]
; the title of the theme
title = Title of the theme
; the order of the tasks (separated by a coma)
task_order = Task2,Task1,Task3
|# This repository is a Demo for [**GenBenchSite**](https://github.com/White-On/BenchSite), a Benchmarking tools for libraries.

## Files organisation

- `project.ini`: Contains the configuration for the project. It is used to configure the website and the benchmarking process.
- `res/`: Contains the resources for the website, such as logos, images, etc. The script will copy the content of this folder to the `pages/assets/` folder.
- `targets/`: Contains the targets to be benchmarked. Each target is a folder containing a `target.ini` file containing the configuration for the target and a `description.html` to describe the target (the file be be added to the page of the target).
- `themes/`: Contains the themes for the task. Each theme is a folder containing a `theme.ini` file containing the configuration for the theme and a `description.html` to describe the theme (the file be be added to the page of the theme).In each theme folder:
    - `task/`: Contains the tasks for the theme. Each task is a folder containing a `task.ini` file containing the configuration for the task and a `description.html` to describe the task (the file be be added to the page of the task).There's also a `extra.html` file that will be added to the page of the task. the `data/` folder is where you can put the data used by your test scripts .Each task folder also contains folders for each target to be benchmarked. Each target folder contains the test scripts for the target. ( a `run` script and a `before` script if needed ). The `evaluation` script is used to evaluate the results of the benchmarking process found in the `output/` folder.


====================
- `pages/`: Contrains the pages of the website generated by the benchmarking process.

## How to use

1. Get the [**GenBenchSite**](https://github.com/White-On/BenchSite) package either by downloading with the command
```pip install genbenchsite``` it or by cloning it. Then you'll have access to the `gbs` command with the help of the command ```gbs help```.
1. Create a template folder for your project with the command ```gbs init <nameBenchmark>```
1. Edit the files to configure your project.
1. Run the benchmarking process with the command ```gbs benchmark -I <pathToTemplateFolder>``` ( the default path is the current directory ). The process will generate the data in the `results.json` folder.
1. Run the website generation process with the command ```gbs website -I <pathToTemplateFolder>``` ( the default path is the current directory ). The process will generate the website in the `pages/` folder.

## Website UI

You'll find the website in the pages folder and is composed of `index.html`, the main pages `content/`, with all the pages generated by the benchmarking process,`script/` for  all the *javascript* files, `style/` with all the *css* files, and `assets/` with all the assets needed for the website.

the main page is `index.html` and should look like this:

You can navigate through the pages using the navigation bar on the left side of the page. The **data** created by the benchmarking process can be downloaded using the **Raw Data** button on the top right of the page.